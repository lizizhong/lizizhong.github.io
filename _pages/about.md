---
permalink: /
title: "Zizhong Li"
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---
Hello!
I am a CS Ph.D. student at the University of California, Davis, advised by [Dr.Jiawei Zhang](http://jiaweizhang.net). Prior to UC Davis, I received my B.E. degree in Computer Science from Tongji University in Shanghai, China. 

My research interest lies in natural language processing, specifically in nonparametric language modeling, misinformation & information retrieval, and multi-modal generative models. 


Publications
------
[06.2024] **Intermediate Distillation: Data-Efficient Distillation from Black-Box LLMs for Information Retrieval**  [arixv](https://arxiv.org/abs/2406.12169)
  
&emsp; **Zizhong Li**, Haopeng Zhang, Jiawei Zhang 

&emsp; In this paper, we introduce *Intermediate Distillation*, A data-efficient knowledge distillation training scheme that treats LLMs as black boxes and distills their knowledge via an innovative LLM ranker-retriever pipeline, solely using LLMs' ranking generation as the supervision signal.

[03.2024] [NAACL2024ðŸŒŸ] **Unveiling the Magic: Investigating Attention Distillation in Retrieval-augmented Generation**  [arxiv](https://arxiv.org/abs/2402.11794)

&emsp; **Zizhong Li**, Haopeng Zhang, Jiawei Zhang

&emsp; In this paper, we address this gap by conducting a comprehensive review of attention distillation workflow and identifying key factors influencing the learning quality of retrieval-augmented language models. We further propose indicators for optimizing models' training methods and avoiding ineffective training.

  



